\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{listings}
\setmarginsrb{1.5 cm}{2.5 cm}{1.5 cm}{2 cm}{1 cm}{1.5 cm}{1 cm}{1.5 cm}

\title{Project 2 Report}								% Title
\author{Pooya Abolghasemi}								% Author
\date{12 Sept 2015}											% Date

\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

\pagestyle{fancy}
\fancyhf{}
\rhead{\theauthor}
\lhead{\thetitle}
\cfoot{\thepage}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
	\centering
    \vspace*{0.5 cm}
    \includegraphics[scale = 0.75]{Images/UCF.png}\\[1.0 cm]	% University Logo
    \textsc{\LARGE University of Central Florida}\\[2.0 cm]	% University Name
	\textsc{\Large CAP6676}\\[0.5 cm]				% Course Code
	\rule{\linewidth}{0.2 mm} \\[0.4 cm]
	{ \huge \bfseries \thetitle}\\
	\rule{\linewidth}{0.2 mm} \\[1.5 cm]

	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \large
			\emph{Submitted To:}\\
			Guo-Jun Qi\\
            Asst. Professor\\
            Computer Science Department\\
			\end{flushleft}
			\end{minipage}~
			\begin{minipage}{0.4\textwidth}

			\begin{flushright} \large
			\emph{Submitted By :} \\
			Pooya Abolghasemi\\
            Fall 2015\\
		\end{flushright}

	\end{minipage}\\[2 cm]

\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\

{The dataset}
I chose the Statlog(Heart) dataset. This database contains 13 attributes (which have been extracted from a larger set of 75). It contains different types of attributes( Real, Binary, Nominal and Ordered). The 13 features are:
\begin{enumerate}
\item age
\item sex
\item chest pain type  (4 values)
\item resting blood pressure
\item serum cholesterol in mg/dl
\item fasting blood sugar more than 120 mg/dl
\item resting electrocardiograph results  (values 0,1,2)
\item maximum heart rate achieved
\item exercise induced angina
\item  oldpeak = ST depression induced by exercise relative to rest
\item the slope of the peak exercise ST segment
\item number of major vessels (0-3) colored by flourosopy
\item  thal: 3 = normal; 6 = fixed defect; 7 = reversible defect
\end{enumerate}

The dataset contains 270 observations and no missing values. The data types as recorded in the Machine Learning Repository is multivariate. The default task is classification and attribute types are categorical and real.\\
For applying logistic regression we need to convert the categorical attributes to a set of boolean attributes. In this dataset we have 3 categorical attribute (number 4, 7 and 13). After converting them to boolean we will end up 20 attributes.
\newpage
\section{Logistic Regression}
For implementing gradient ascend logistic regression we need the conditional log likelihood function
\begin{equation}\label{likelihood}
	l(W)=\sum\limits_{l}Y^l(w_0+\sum\limits_{i}^n w_iX_i^l)-ln(1+exp(w_0+\sum\limits_{i}^n w_iX_i^l))
\end{equation}
and we need to calculate the derivative of log likelihood function for gradient ascend algorithm.
\begin{equation}\label{derivative}
	\frac{\partial l(W)}{\partial w_i} = \sum\limits_{l} X_i^l(Y^l-\hat{P}(Y^l=1|X^l,W))
\end{equation}
Based on these equation we are going to update the logistic regression weights based on these equation:
\begin{equation}\label{update_rule}
	w_i \leftarrow w_i + \eta\sum\limits_{l} X_i^l(Y^l-\hat{P}(Y^l=1|X^l,W)
\end{equation}

In my setting for running batch gradient ascent I repeat updating the weights until the log likelihood difference between two successive iterations goes below a certain threshold. However, for running stochastic gradient ascent I used the maximum value for the number of iteration. Because log likelihood difference between two successive iteration in stochastic setting might get very small in early iterations because we are not considering all the data. I am going go through the details in the following chapters.

\section{Logistic Regression - Batch Gradient Ascent}
For this part I considered 65\% of the dataset as training set and the remaining as test set. The learning rate was set to 0.015.
Figure\ref{fig:batch_loglikelihood} shows the ascending of log likelihood in successive iterations for batch gradient ascending.
\begin{figure}
  \caption{log likelihood vs iteration}
  \centering
  \includegraphics[width=0.7\textwidth]{Images/likelihood_plot_batch.png}
  \label{fig:batch_loglikelihood}
\end{figure}
Figure\ref{fig:batch_train_test} shows the train and test error for batch gradient ascent. The error is simply the difference between the model prediction and the real label for each of the samples divided by number of samples.
\begin{figure}
  \caption{train error(Blue) and test error(red) for batch gradient ascent}
  \centering
  \includegraphics[width=0.7\textwidth]{Images/train_test_error_batch.png}
  \label{fig:batch_train_test}
\end{figure}
I could not see any over-fitting in the process even when I let the algorithm to perform 1000 iterations. The only time that I saw over-fitting was when I decrease training data dramatically. I decreased the training data to 10\% and you can see the train and test error in figure\ref{fig:batch_train_test_overfit}. In this case obviously lack of training data caused the over-fitting. I needed this case to see whether my regularization works or not, since without over-fitting I could not see any benefit when I added the regularization term to the likelihood function.
\begin{figure}
  \caption{train error(Blue) and test error(red) for batch gradient ascent}
  \centering
  \includegraphics[width=0.7\textwidth]{Images/train_test_error_batch_overfit.png}
  \label{fig:batch_train_test_overfit}
\end{figure}
\newpage
\section{Logistic Regression - Stochastic Gradient Ascent}
For stochastic gradient ascending we need to choose a proper size of randomly chosen subset. To do so I test the process with different subset sizes. In figure\ref{fig:stochastic_subset_size} you can see train and test error with different values for the subset size. After observing results for different subset sizes between 1 to 100 I set the subset size to 40. Since subset sizes bigger than 40 doesn't improve the performance very much. As it is shown in the graphs bigger subset size gives us better performance but as a result the process take more time especially when subset size is very big. When the subset size is not big enough and there is lots of outliers or inaccurate measurements for the attributes in the dataset, stochastic gradient ascent might take a long time to converge. However, stochastic gradient ascent is useful for the datasets with lots of samples, since performing batch gradient ascent will take lots of time.
\begin{figure}
  \caption{train error(Blue) and test error(red) for stochastic gradient ascent. Subset size grow from left to right, top to bottom}
  \centering
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_stochastic_1.png}
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_stochastic_41.png}
  \includegraphics[width=0.5\textwidth]{Images/train_test_error_stochastic_81.png}
  \label{fig:stochastic_subset_size}
\end{figure}
In figure \ref{fig:stochastic_loglikelihood} you can see the log likelihood of stochastic gradient ascent with 40 subset size.
\begin{figure}
  \caption{likelihood vs iterations for stochastic gradient ascent with 40 subset size}
  \centering
  \includegraphics[width=0.7\textwidth]{Images/likelihood_plot_stochastic_41.png}  		\label{fig:stochastic_loglikelihood}
\end{figure}
\newpage
\section{Logistic Regression - Batch Gradient Ascent Regularized}
For regularizing logistic regression we need to add the term $-\frac{1}{2\sigma^2}||w||^2$. So our equations will look like this:
\begin{equation}\label{likelihood_reg}
	l(W)=\sum\limits_{l}Y^l(w_0+\sum\limits_{i}^n w_iX_i^l)-ln(1+exp(w_0+\sum\limits_{i}^n w_iX_i^l))-\frac{1}{2\sigma^2}||w||^2
\end{equation}
\begin{equation}\label{derivative_reg}
	\frac{\partial l(W)}{\partial w_i} = \sum\limits_{l} X_i^l(Y^l-\hat{P}(Y^l=1|X^l,W))-\frac{w_i}{\sigma}
\end{equation}
\begin{equation}\label{update_rule_reg}
	w_i \leftarrow w_i + \eta\left[\sum\limits_{l} X_i^l(Y^l-\hat{P}(Y^l=1|X^l,W)-\frac{w_i}{\sigma}\right]
\end{equation}
For the regularization section I divided the dataset into three groups: train set(60\%), validation set(20\%) and test set(20\%). I tried different values for $\sigma$. it is set to $[0.1, 0.5,1,2,10,1000]$. The purpose was to tune $\sigma$ over validation set. However, Since I did not have any over-fitting on my model, setting $\sigma$ to bigger values had a better result for my model. I also test the the regularization in the case that I set the training set to be very small, just to see the regularization effect.\\
These figures show regularized classifiers with different values for $\sigma$.
bigger value for the $\sigma$ will reduce the effect of regularization. As it is illustrated in the graphs since we did not have any over-fitting on the data regularization does not help us that much, it just make the graphs more smooth. In the figure with $\sigma$ set to 0.1, since $\eta/\sigma$ is equal to $0.2$ it means that we subtract $0.2w_i$ from it each iteration and this have a severe impact on the results and the test error seem to fluctuate.
\begin{figure}
  \caption{batch gradient ascent with regularization with different values for $\sigma$. From left to right and top to bottom $\sigma$ is set to $0.1, 0.5, 10, 100000$.}
  \centering
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_batch_0_1.png}
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_batch_0_5.png}
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_batch_10.png}
    \includegraphics[width=0.49\textwidth]{Images/train_test_error_batch_100000.png}
  \label{fig:batch_reg}
\end{figure}
Now I am going to investigate the case when the training set is set to $10\%$ and we have over-fitting. In figure \ref{over_fitting} train(blue) and test(red) error is shown for cases with and without regularization. Obviously, lack of training samples caused over-fitting in this case. I am describing this just to show the regularization effect. In the right graph (with regularization) you can see that test error remain at same level (around 0.3) compare to left graph (without regularization) which test error goes up (around 0.4) by performing more iterations.
\begin{figure}
  \caption{batch gradient ascent with(right) and without(left) regularization}
  \centering
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_batch_overfit.png}
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_batch_10_overfit.png}
  \label{over_fitting}
\end{figure}
\newpage
\section{Logistic Regression - Stochastic Gradient Ascent Regularized}
Figure \ref{fig:stochastic_reg} shows regularized stochastic gradient ascent with different values for $\sigma$. As discussed in the previous section since the model does not have over-fitting, regularization with higher values for seems to have a better performance.
\begin{figure}
  \caption{stochastic gradient ascent with regularization with different values for $\sigma$. From left to right and top to bottom $\sigma$ is set to $0.1, 0.5, 10, 1000$.}
  \centering
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_stochastic_0_1.png}
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_stochastic_0_5.png}
  \includegraphics[width=0.49\textwidth]{Images/train_test_error_stochastic_10.png}
    \includegraphics[width=0.49\textwidth]{Images/train_test_error_stochastic_1000.png}
  \label{fig:stochastic_reg}
\end{figure}
\newpage
\section{Compare With Decision Tree}
Results shows
\centering
\begin{tabular}{| c | c | c | c | c | }
  \hline
   Test Fold Index& Accuracy & Precision & Recall & F1-Measure \\
  \hline
      Decision Tree& 70.92\%  & 69.36\% & 61.7\% & 0.64  \\
  \hline
      Batch Gradient Ascent& 88.42\%  & 80\% & 91.42\% & 0.85  \\
  \hline
      Stochastic Gradient Ascent& 82.1\%  & 80.5\% & 74.3\% & 0.77  \\
  \hline
\end{tabular}
\end{document}
